---  
layout: post
title: ููุฎุต ููุฑุณ ุนูู ุงูุจูุงูุงุช - 10
icon: ๐
---  

ูุฐุง ุงูููุฎุต ุนู ููุชุจุฉ PyTorchุ ููุชุจุฉ ุฃุฎุฑู ูุชุฎุตุตุฉ ููุดุจูุงุช ุงูุนุตุจูุฉ ูู ุจุงูุซูู.  
  
  


##### ุงููุตู ุงูุซุงูู - ุงูุชุนูู ุงูุนููู Deep Learning  
##### ุงูุฏุฑุณ ุงูุฎุงูุณ - Deep Learning with PyTorch  
ุงูุฏุฑุณ ุนููู ุจุดูู ูุงููุ ูุฐุง ุณุฃุถุน ุจุนุถ ุงูุฃูุซูุฉ ูุทุฑููุฉ ุฅูุดุงุก ุดุจูุงุช ุนุตุจูุฉ ูู PyTorch.  

##### ุนูููุฉ ุงูุญุณุงุจ  
![](https://alioh.github.io/images/2019-4-1/perceptron.png)  

ุทุฑููุฉ ุงูุญุณุงุจ (ูุซุงู ุนูู ุงูุตูุฑุฉ ุงูุนูููุฉ) ูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูุงูุชุงูู: 
* ูุฏููุง ูู ุงููLayer ุงูุฃูู ุซูุงุซ ูุฏุฎูุงุช Features ( Inputs ) ( x1, x2, x3 ) ุฅุถุงูุฉ ุฅูู ุงููBias ููู 1 ููุง.  
* ูู ูุฏุฎู ูุฏูุฉ Weight ( w1, w2, w3, w4 ).  
* ูุถุฑุจ ูู Input ุนูู ูู Weight ููุชูู ูู ( x1 * w1, x2 * w2, x3 * w3 )
* ุชุฌูุน ูุชุงุฆุฌ ุนูููุงุช ุงูุถุฑุจ ุงูุณุงุจูุฉ ููุถุงู ุฅูููุง ุงููbias (b) ูุชุทุจู ุนูููุง ุฏุงูุฉ ูุนููุฉ ูุซู Sigmoid ูุชุฎุฑุฌ ููุง ูุชูุฌุฉ.  
* ุฏุงูุฉ Sigmoid:
    * ![](https://alioh.github.io/images/2019-4-1/sigmoid-equation.png)  
    * ูุถูู ุงููุชูุฌุฉ ุงูุชู ุญุตููุง ุนูููุง ููุง ุจุฏูุงู ูู x ูู ุงูุฏุงูุฉ ุงูุณุงุจูุฉ ููู ุงููุชูุฌุฉ ุงูููุงุฆูุฉ ููุดุจูุฉ  .

##### Tensor  
![](https://alioh.github.io/images/2019-4-1/tensor.png)  
ูู ูุง ูููู ููุฏุฎู ูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ุนุจุงุฑุฉ ุนู Tensorุ ุจูุนูุง ุฃุตุญุ ุงู ุดุฆ ูุฑูุฏ ุฅุฎุชุจุงุฑุฉ ูู ุงูุดุจูุฉ ูุฌุจ ุฃู ูุญููุฉ ุฅูู Tensor ูู ุฃุฑูุงู.  

##### ุทุฑููุฉ ุชุตููู ุดุจูุฉ ุนุตุจูุฉ ูู Pytorch  
ุดุจูุฉ ุนุตุจูุฉ ุจูLayer ูุงุญุฏ.  
```python 
import torch

#   ุฏุงูุฉ Sigmoid
def activation(x):
    """ Sigmoid activation function 
        ุงููุฏุฎู ููุง ูู ุฏุงุฆูุงู ูุชูุฌุฉ ุงูุดุจูุฉ
        ---------
        x: torch.Tensor
    """
    return 1/(1+torch.exp(-x))

#   ุจูุง ุงููุง ุณูููู ุจุชุตููู ุดุจูุฉ ุนุตุจูุฉ ูู ุงุฑูุงู ุนุดูุงุฆูุฉ
#   ูุฑูุฏ ูุน ูู ุชุดุบูู ููุดุจูุฉ ุชููู ุงูุฃุฑูุงู ููุณูุฉ ููุชุญูู
#   ูู ูุชุงุฆุฌ ุจูุงูุงุช ุงูุชุฏุฑูุจ ูุงูุฅุฎุชุจุงุฑุ ูุฐุง ูุถุนูุง ูุฐู ุงูุฏุงูุฉ manual_seed
torch.manual_seed(7) 

#   ูุฐุง ุงูุฃูุฑ ุณูููู Tensor ุจุนุงููุฏ ูุงุญุฏ ูุฎูุณ ุงุณุทุฑ
features = torch.randn((1, 5))

#   ุณูุฌุนู ุงูุจุฑูุงูุฌ ูููู ููุง Weight ุจุดูู ุนุดูุงุฆู
#   ูููู ุณุชููู ุจููุณ ููุงุณุงุช ุงููุฏุฎูุงุช ูุฏููุง ููุณุชุฎุฏู randn_like ูุชูููููุง
weights = torch.randn_like(features)
#   ูุฐูู ุงู bias ูููู ุณุชููู ูููุตูุฉ ุนู ุงูุจููู ูู Tensor ูุญูุฏุฉ 1ร1
bias = torch.randn((1, 1))

#   ููููุงู ุจุงูุนูููุฉ ุงูุญุณุงุจูุฉ ุงูุชู ุดุฑุญุชูุง ูุณุจูุงู ูููู ุจุงูุชุงูู
#   ุนูููุฉ ุงูุถุฑุจ features * weights
#   ุงูููุชุจุฉ pytorch ุจููุณูุง ุนูุฏูุง ุชููู ุจุนูููุฉ ุงูุถุฑุจ ุชุถุฑุจ ูู ูููุฉ ุจูุง ููุงุจููุง
#   x1 * w1 , x2 * w2 .. etc
#   ูุฌูุน ูู ูุชุงุฆุฌ ุนูููุงุช ุงูุถุฑุจ ุจูุงุณุทุฉ .sum()
#   ุซู ูุถูู ููุง ุงูbias ููููู ุจุญุณุงุจ Sigmoid ูู ุงูุฏุงูุฉ ุงูุชู ุณุจู ุงู ุนุฑููุงูุง
y = activation((features * weights).sum() + bias)
#   ุฃู
#   y = activation(torch.sum(features * weights) + bias)
#   ููุง ุงูุฎุทูุชูู ุตุญูุญุฉ
print(y)

#   ูููู ุฅุฎุชุตุงุฑูุง ุฃูุซุฑ ุฃูุถุง ุนู ุทุฑูู matrix multiplication
#   ุจูุงุณุทุฉ torch.mm() ูุงูุชู ุชููู ุจุนูููุฉ ุงูุถุฑุจ ูุงูุฌูุน ูู ุฎุทูุฉ ูุงุญุฏุฉ
#   ููุฐูู ูุญุชุงุฌ ูุชุบูุฑ ุดูู ุงููWeight ุจุฏูุงู ูู ุฃู ุชููู 1ร5 (ุนุงููุฏ ูุงุญุฏ ูุฎูุณ ุงุณุทุฑ)
#   ููููู 5ร1ุ (ุฎูุณ ุนูุงููุฏ ูู ุณุทุฑ ูุงุญุฏ)
#   ุชูุฌุฏ ุซูุงุซ ุทุฑู ููุชุญููู ูู pytorch:
new_weights = weights.view(5,1) #   ุฃู weights.reshape(5, 1) ุฃู weights.resize_(5, 1)
#   ูููุชุฃูุฏ ุฃููุง ูุญุตู ุนูู ููุณ ุงููุชุงุฆุฌ ุจุงูุนูููุฉ ุงูุณุงุจูุฉ ูููู ุจุฅุฎุชุจุงุฑูุง ูุงูุชุงูู
print(activation(torch.mm(features, new_weights) + bias))
```

ูุซุงู ุขุฎุฑ ูุดุจูุฉ ุนุตุจูุฉ ูุถุงู ููุง Hidden Layer ูููู ูู ูุฏุฎููู ุงุซููู ูุดููุฉ ูุงูุชุงูู:  
![](https://alioh.github.io/images/2019-4-1/2hiddeninputs.jpg)  
```python
#   ุจููุณ ุงูุทุฑููุฉ ุจุงููุซุงู ุงูุณุงุจู ููุดุฃ ุงูุดุจูุฉ ููุนุทููุง ูุฏุฎูุงุช
torch.manual_seed(7)
features = torch.randn((1, 3))

#   ูุญุฏุฏ ููุงุณุงุช ูู layer

#   ุนุฏุฏ ุงููุฏุฎูุงุช ูุฌุจ ุฃู ูุณุงูู ุนุฏุฏ ุงูfeatures ููุง 3
n_input = features.shape[1]     
#   ุนุฏุฏ ุงููุฏุฎูุงุช ูู ุงูHidden Layer (ุงูุฃุฎุถุฑ ูู ุงูุตูุฑุฉ ุงูุนูููุฉ)
n_hidden = 2                    
#   ุนุฏุฏ ุงููุชุงุฆุฌ ุฃู ุงููุฎุฑุฌุงุช
n_output = 1


#   ูุญุฏุฏ ุงููWeights ููุฌุฒุก ุงูุฃูู ูู ุงููุฏุฎูุงุช ุฅูู Hidden Layer
W1 = torch.randn(n_input, n_hidden)
#   ููู ุงูHidden Layer ุฅูู ุงููุฎุฑุฌุงุช
W2 = torch.randn(n_hidden, n_output)

#   ูุฐู ุงูbias ูููุง ุงูุฎุทูุชูู
B1 = torch.randn((1, n_hidden))
B2 = torch.randn((1, n_output))

#   ููุญุณุจ ุงููุชูุฌุฉ ูุงูุชุงูู
hidden_layer = activation(torch.mm(features, W1) + B1)
y = activation(torch.mm(hidden_layer, W2) + B2)
print(y)
```

ููุชุญููู ูู ูุตูููุฉ ูู numpy ุฅูู tensor ูู pytorch ูููู ุจุงูุชุงูู
```python
import numpy as np
a = np.random.rand(4,3)
#   ุงูุขู b ูู tensor ุจููุณ ุดูููุง ูู numpy
b = torch.from_numpy(a)
print(b)
#   ููุนุฑุถ ุดูููุง ููุง ูู numpy
print(b.numpy())
```

ูุฐุง ูุซุงู ุจุณูุท ูุทุฑููุฉ ุชุดููู ุดุจูุฉ ุนุตุจูุฉ ูุญุณุงุจ ูุชุงุฆุฌูุง. ูู ุงูุฃูุซูุฉ ุนูู ููุณ ุงูุจูุงูุงุช:  
* [TowardsDataScience](https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627)  
* [gkoehler](https://nextjournal.com/gkoehler/pytorch-mnist)  
* [athul929](https://medium.com/@athul929/hand-written-digit-classifier-in-pytorch-42a53e92b63e)  

##### NN in Pytorch  
ุชุณูู Pytorch ุนูููุฉ ุฅูุดุงุก ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุนูุฏ ุฅุณุชุฎุฏุงู ููุชุจุฉ nn ุงูุชุงุจุนุฉ ููุง. ูุซุงู:  
```python 
from torch import nn
import torch.nn.functional as F

class Network(nn.Module):
    def __init__(self):
        #   ูุญุชุงุฌ ุงูุณุทุฑ ุงูุฃูู ุฏุงุฆูุงู ุฅุฐุง ุฃุฑุฏูุง ุงูุนูู ุจูุฐู ุงูุทุฑููุฉุ ูู ูุชุนุฑู Pytorch
        #   ุนูู ุงููุง ููุตูู ุดุจูุฉ ุนุตุจูุฉ ุจุนุฏุฉ layers
        super().__init__()

        #   Input ---> hidden layer
        self.hidden = nn.Linear(784, 256)
        #   Hidden ---> Output layer
        self.output = nn.Linear(256, 10)
        
    def forward(self, x):
        #   ูุฑุฑูุง ูุชูุฌุฉ ุงูู input ---> hidden layer ุฅูู ุฏุงูุฉ sigmoid
        x = F.sigmoid(self.hidden(x))
        #   ูุฑุฑูุง ูุชูุฌุฉ ุงูู hidden ---> output layer ุฅูู ุฏุงูุฉ sigmoid
        x = F.softmax(self.output(x), dim=1)

        return x
```

ูุซุงู ุขุฎุฑ:  
![](https://alioh.github.io/images/2019-4-1/mlp_mnist.png)  
ุทุฑููุฉ ุชูููู ูุฐู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูู ุจุงูุซูู ู pytorch ูุงูุชุงูู:
```python
from torch import nn
import torch.nn.functional as F

class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.h1 = nn.Linear(784, 128)
        self.h2 = nn.Linear(128, 64)
        self.output = nn.Linear(64, 10)
        
    def forward(self, x):
        x = F.relu(self.h1(x))
        x = F.relu(self.h2(x))
        x = F.softmax(self.output(x), dim=1)
        
        return x
model = Network()
model
```

ูู ุงูููุฎุต ุงูุณุงุจู ุงุณุชุฎุฏูุช Sequentialุ ููู ููุณูุง ุณูุณุชุฎุฏููุง ููุงุ ูุชูููู ุดุจูุฉ ุนุตุจูุฉ ูุงููุฉุ ูุทุฑููุชูุง ูุงูุชุงูู

```python
#   ูุนุฑู ุงููุฏุฎูุงุช ูุญุฌููุง
input_size = 784
hidden_sizes = [128, 64]
output_size = 10

#   ููุง ุนุฑููุง Sequential ูุจุฏุฃูุง ุจุฏุงุฎููุง ุจุชุนุฑูุจ ุงููLayers
#   ูุงุญุฏ ุชูู ุงูุขุฎุฑ ููุง ูู ุงูุฃูุซูุฉ ุงูุณุงุจูุฉ.
model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[1], output_size),
                      nn.Softmax(dim=1))
print(model)
```

##### ุชุฏุฑูุจ ุงูุดุจูุฉ ุงูุนุตุจูุฉ  
* **Loss function**: ุญุณุงุจ ูุณุจุฉ ุงูุฎุทุฃ ูู ุงูุชููุนุงุช. ูุญุงูู ุชุฏุฑูุจ ุงูุดุจูุฉ ุฏุงุฆูุงู ุนูู ุฃู ุชููู ูุชุงุฆุฌ ุงููLoss Function ูููุง ุฃูู ูุง ูููู. ูู ูุง ูู ูู ูุง ูุงู ุงูููุฏู ุฃูุถู.  
* **Gradient Descent**: ุณุจู ุชุญุฏุซุช ุนููุง ูู ุงูุฌุฒุก [ุงูุฃูู](https://alioh.github.io/DSND-Notes-1/)ุ [ุงูุฑุงุจุน](https://alioh.github.io/DSND-Notes-4/) ู[ุงูุซุงูู](https://alioh.github.io/DSND-Notes-8/)ุ ุชุณุชุฎุฏู ูู ุงูุดุจูุงุช ุฐุงุช ุงูุชู ุชุญุชูู ุนูู Layer ูุงุญุฏ ููุท.  
* **Backpropagation**: ุจุนุฏ ูุฑูุฑ ุงูุจูุงูุงุช ุนูู ุงูุดุจูุฉ ุงูุนุตุจูุฉ ูุงูุญุตูู ุนูู ูุชูุฌุฉ ุงููLossุ ุชุนูุฏ ุจููุณ ุงูุทุฑููุฉ ุฅูู ุงููWeight ูููุฏุฎูุงุช ูุชุญุงูู ุงูุชูููู ููู ููุญุตูู ุนูู ุฃูู ูุชูุฌุฉ ูููLoss. ุชุณุชูุฑ ุจุนูู ูุฐู ุงูุฎุทูุงุช ุญุชู ุชุตู ูุฃูู Loss.  

##### Activation Functions  
ุญุชู ุงูุขู ุงุณุชุฎุฏููุง ููุนููุ Sigmoid / Softmax. ุชูุฌุฏ ุฃููุงุน ุงุฎุฑู ูุซู Tanh ู ReLu.  [^1] [^2]  


ุชุทุจููุงุช ุนูููุฉ ูุงููุฉ ูุน ุงูุญููู ุชุฌุฏูููุง [ููุง](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch)ุ ุงูุฑุงุจุท ุฎุงุต ุจุงูููุฑุณ ููุญุชูู ุนูู ุงูุซูู ูุดุฑูุญุงุช ูู ุฃุจุณุท ุงูุดุจูุงุช ุงูุนุตุจูุฉ ุฅูู ุดุจูุงุช ุนุตุจูุฉ ูุงุฏุฑุฉ ุนูู ุชููุน ูุฅุธูุงุฑ ูุชุงุฆุฌ ููุตูุฑ. ุฌููุน ุงูุฃูุซูุฉ ูู ุตูุญุงุช jupyter.  

ุฃูุซููุฉ ุนูููุฉ ุนุงูุฉ: [1](https://jhui.github.io/2017/03/18/Deep-learning-tutorial/)ุ [2](https://jhui.github.io/2017/03/17/Deep-learning-tutorial-2/)

-----
[ุงูุนูุฏุฉ ุฅูู ููุฎุต ููุฑุณ ุนูู ุงูุจูุงูุงุช - 9](https://alioh.github.io/DSND-Notes-9/)   -   [ุงูุฅูุชูุงู ุฅูู ููุฎุต ููุฑุณ ุนูู ุงูุจูุงูุงุช - 11](https://alioh.github.io/DSND-Notes-11)  
  
  
[^1]: [TowardsDataScience](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
[^2]: [TowardsDataScience](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)