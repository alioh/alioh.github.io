---  
layout: post
title: تعلم الآلة للجميع - 4
---  

بعد أن تعمقنا قليلاً في [التعلم الموجَّه](https://alioh.github.io/Machine-Learning-for-Everyone-3/) وخوارزمياته، سأنتقل للتحدث عن النوع الآخر من طرق تعلم الآلة وهو التعلم غير الموجَّه.



##### التعلم غير الموجَّه (Unsupervised Learning)  
هي قدرة الآلة على التعلم بنفسها بدون الحاجه لتدخل، وبدون إعطائها نتائج مسبقة. وتوجد طريقتين للتعلم الغير موجَّه:
* **Clustering التجميع**: جمع البيانات على شكل فئات حسب خصائص معينه تتاشبه فيها.
* **Association Rules قواعد / قوانين الربط**: ربط البيانات فيما بينها عن طريق محاولة أكتشاف علاقات بينها.  

![](https://alioh.github.io/images/2019-2-12/1.png)  


###### (Clustering) التجميع  
تقوم هذه الطريقة بتقسيم البيانات إلى أجزاء أصغر، بشرط أن تكون كل مجموعة لديها خصائص متشابهه. وتسمى كل مجموعة بـ (Cluster). وتستخدم هذه الطريقة كثيراً في التسويق، حيث تقوم الخوارزميات بتقسيم الأشخاص إلى مجموعات حسب خصائص معينة مثل الدخل الشهري أو العمر، وتتيح هذه الطريقة للشركات تسويق منتجات معينه لأشخاص معينين.  
ومن أكثر الخوارزميات إستخدماً في التجميع:  


* **(K-mean Clustering) تجميع بالمتوسطات**: [^2]  

![](https://alioh.github.io/images/2019-2-12/2.png)  

  * يتم إعطاء الخوارزمية رقم (K) ويعني عدد مرات تقسيم (تجميع) البيانات إلى (Clusters).
  * بنفس الرقم K يتم رسم نقاط تعرف بالنقاط المركزية (Centroids).
  * تربط كل بيانات بما يقارب لها من نقاط مركزية.
  * تقوم الخوارزمية بحساب المتوسط لجميع النقاط داخل كل (Cluster).
  * تكرار الخطوتين الأخيرتين مرة أخرى حتى تصل لنقاط مركزية وتجميعة مناسبة لجميع النقاط.  
    
  [مثال](https://healthcare.ai/step-step-k-means-clustering/)  


* **(Hierarchical Clustering) تجميع هرمي**: [^3]  
![](https://alioh.github.io/images/2019-2-12/3.gif)  
الرسم البياني يوضح كيفية عمل الخوارزمية، وتسمى الهرمية لأنها تبدأ من الأسفل إلى الأعلى، طريقة إتخاذ القرار فيها يتم كالتالي:  
  * على عكس التجميع بالمتوسطات (K-mean)، التجميع الهرمي يُكَوِّن مجموعة أو (Cluster) لكل نقطة لدينا.
  * يتم جمع أقرب نقطتين مع بعضهم البعض وتُكَون مجموعة جديدة.
  * تبحث الخوارزمية عن نقطة أخرى قريبة لها وتضمها إلى نفس المجموعة (Cluster).
  * تستمر الخوارزمية بعمل الخطوتين السابقتين إلى أن تبقى مجموعة واحده (Cluster) تُجمع فيها كل النقاط.  
    
  [مثال](https://towardsdatascience.com/hierarchical-clustering-and-its-applications-41c1ad4441a6)  

##### Association Rules قواعد / قوانين الربط  
لتوضيح فكرة الربط هنا، لنأخذ مثلاً في المتاجر الكبيره، يتم وضع الخضروات في نفس الصف، ومنتجات الألبان في صف آخر، وهكذا مع كل المنتجات. يتم وضعها بهذه الطريقة ليس فقط لإختصار الوقت على المشتري، ولكن أيضاً لعرض منتجات مشابهه بالقرب لمنتج يبحث عنه، بهذه الطريقة قد يتم جذب المشتري لمنتجات أخرى لديها خصائص مشابهه للمنتج المطلوب. ولقياس ما إذا كان منتجين مربوطين ببعضهم البعض نعتمد على ثلاث معايير:  
  * **(Support)**: عدد مرات تكرار المنتج في مجموعه من البيانات.
  * **(Confidence)**: إحتمالية شراء مُنتج "أ" مع مُنتج "ب".
  * **(Lift)**: نسبة إحتمالية شراء المنتج "أ" مع "ب".  
وعملية حساب كل قيمة هي كالتالي:  
![](https://alioh.github.io/images/2019-2-12/4.png)  

قد يبدو الموضوع مُبهم بعض الشئ خاصة في آخر نقطتين، لكن سيتضح الأمر في المثال التالي. لنفترض أن لدينا عربة التسوق هذه:   [^4]

![](https://alioh.github.io/images/2019-2-12/5.png)  
لدينا المنتج A ونريد أن نحدد ما إذا كان له علاقة بالمنتج C، كما ذكرنا في النقطة السابقة، تقوم الخوارزمية بالإعتماد على ثلاث معايير:  
  * **(Support)**: ونجدها بحساب عدد مرات تكرار المنتجين معاً 2 ونقسمها على مجموع العربات 5.
  * **(Confidence)**: تُحسب بقسمة عدد تكرار المنتجين معاً 2 على عدد مرات تكرار المنتج الاول A وهو 3.
  * **(Lift)**: تساوي قسمة نتيجة (Support) المنتجين على ضرب نتيجة (الـSupport) لكل منتج على حده. هنا نتيجة الـ (Support) للمنتج A هي 0.6 و المنتج (C) هي 0.8. النتيجة النهائية للـ(Lift) تساوي نتيجة (Support) لدينا وهي 0.4 تقسم على نتيجة (الـSupport) لكل منتج على حدة، للمنتج A النتيجة 0.6 وللمنتج (C) النتيجة 0.8. النتيجة النهائية للـLift هي: 0.4 ÷ (0.6 * 0.8) = 0.833

من الخوارزميات التي تنحدر أسفل (Association Rules) هي (Apriori Algorithm)، تقوم الخوارزمية بإكتشاف القيم الأكثر تكراراً. وتعمل كالتالي:  [^4]

![](https://alioh.github.io/images/2019-2-12/6.jpg)  

  * لنفترض أننا وضعنا الحد (Threshold) يساوي 1.
  * نحسب تكرار كل قيمه.
  * يتم جمع القيم الأعلى من العملية الأخيره مع باقي القيم التي مجموع تكرارها أكبر من 1.
  * نكرر العملية الأخيره مره أخرى إلى ان يبقى لنا قيمة واحده.   
توجد إستخدامات كثيره للخوارزمية وليست فقط محدودة على المحلات التجارية، يمكن أستخدامها في مواقع مثل (Netflix) لتقديم إقتراحات المشاهدة.  

مثال آخر [هنا](https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html/2) أو [هنا](https://www.hackerearth.com/blog/machine-learning/beginners-tutorial-apriori-algorithm-data-mining-r-implementation/).  



[^1]: <https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257>
[^2]: <https://towardsdatascience.com/clustering-unsupervised-learning-788b215b074b>
[^3]: <https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68>
[^4]: <https://www.saedsayad.com/association_rules.htm>
