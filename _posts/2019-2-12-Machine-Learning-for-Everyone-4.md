---  
layout: post
title: تعلم الآلة للجميع - 4
published: false
---  

بعد أن تعمقنا قليلاً في [التعلم الموجَّه](https://alioh.github.io/Machine-Learning-for-Everyone-3/) وخوارزمياته، سأنتقل للتحدث عن النوع الآخر من طرق تعلم الآلة وهو التعلم غير الموجَّه.



##### التعلم غير الموجَّه Unsupervised Learning  
هي قدرة الآلة على التعلم بنفسها بدون الحاجه لتدخل، وبدون إعطائها نتائج مسبقة. وتوجد طريقتين للتعلم الغير موجَّه:
* **Clustering التجميع**: جمع البيانات على شكل فئات حسب خصائص معينه تتاشبه فيها.
* **Association Rules قواعد / قوانين الربط**: ربط البيانات فيهما بينها عن طريق محاولة أكتشاف علاقات بينها.  
![](https://alioh.github.io/images/2019-2-13/1.png)  


###### Clustering التجميع  
تقوم هذه الطريقة بتقسيم البيانات إلى اجزاء اصغر، بشرط ان تكون كل مجموعة لديها خصائص متشابهه. وتسمى كل مجموعة بـ Cluster. وتستخدم هذه الطريقة كثيراً في التسويق، حيث تقوم الخوارزميات بتقسيم الأشخاص إلى مجموعة حسب خصائص معينة مثل الدخل الشهري أو العمر، وتتيح هذه الطريقة للشركات تسويق منتجات معينه لأشخاص معينين.  
ومن أكثر الخوارزميات إستخدماً في التجميع:  


* **K-mean Clustering تجميع بالمتوسطات**: [^2]  
![](https://alioh.github.io/images/2019-2-13/2.png)  
  * ويتم إعطاء الخوارزمية رقم K ويعني عدد مرات تقسيم (تجميع) البيانات إلى Clusters.
  * بنفس الرقم K يتم رسم نقاط تعرف بالنقاط المركزية Centroids.
  * تربط كل بيانات بما يقارب لها من نقاط مركزية.
  * يتم القيام بعملية حساب بين النقطة المركزية والنقاط التي حولها وتحديد مكان جديد للنقطة المركزية يكون مناسب لجميع النقاط الأخرى.
  * تكرار الخطوتين الأخيرتين مرة أخرى حتى تصل لنقاط مركزية وتجميعة مناسبة.  


* **Hierarchical Clustering تجميع هرمي**: [^3]  
![](https://alioh.github.io/images/2019-2-13/3.gif)  
الرسم البياني يوضح كيفية عمل الخوارزمية، وتسمى الهرمية لأنها تبدأ من الأسفل إلى الآعلى، طريقة إتخاذ القرار فيها يتم كالتالي:  
  * على عكس التجميع بالمتوسطات K-mean، التجميع الهرمي يُكَون مجموعة أو Cluster لكل نقطة لدينا.
  * يتم جمع أقرب نقطتين مع بعضهم البعض وتُكَون مجموعة جديدة.
  * تبحث الخوارزمية عن نقطة أخرى قريبة لها وتضمها إلى نفس المجموعة Cluster.
  * تستمر الخوارزمية بعمل الخطوتين السابقتين إلى أن تبقى مجموعة واحده Cluster يُجمع فيها كل النقاط.  


##### Association Rules قواعد / قوانين الربط  
لتوضيح فكرة الربط هنا، لنأخذ مثلاً في المتاجر الكبيره، يتم وضع الخضروات في نفس الصف، والمنتجات الألبان في صف آخر، وهكذا مع كل المنتجات. يتم وضعها بهذه الطريقة ليس فقط لإختصار الوقت على المشتري، ولكن أيضاً لعرض منتجات مشابهه بالقرب لمنتج يبحث عنه، بهذه الطريقة قد يتم جذب المشتري لمنتجات أخرى لديها خصائص مشابهه للمنتج المطلوب. ولقياس ما إذا كان منتجين مربوطين ببعضهم البعض نعتم على ثلاث معايير:  
  * **Support**: عدد مرات تكرار المنتج في مجموعه من البيانات.
  * **Confidence**: تعني إحتمالية شراء مُنتج "أ" مع مُنتج "ب".
  * **Lift**: نسبة إحتمالية شراء المنتج "أ" مع "ب".  
وعملية حساب كل قيمة هي كالتالي:  
![](https://alioh.github.io/images/2019-2-13/4.png)  

قد يبدو الموضوع مُبهم بعض الشئ خاصة في آخر نقطتين، لكن سيتضح الأمر في المثال التالي. لنفترض أن لدينا عربة التسوق هذه:   [^4]

![](https://alioh.github.io/images/2019-2-13/5.png)  
لنفترض أن لدينا المنتجى A ونريد أن نحدد ما إذا كان له علاقة مع المنتج C، كما ذكرنا في النقطة السابقة، تقوم الخوارزمية بحساب الثالث معايير كالتالي:  
  * **Support**: ونجدها بحساب عدد مرات تكرار المنتجين معاً 2 ونقسمها على مجموع العربات 5.
  * **Confidence**: تُحسب بقسمة عدد تكرار المنتجين معاً 2 على عدد مرات تكرار المنتج الاول A وهو 3.
  * **Lift**: تساوي قسمة نتيجة Support المنتجين على ضرب نتيجة الـSupport لكل منتج على حده. هنا نتيجة الـ Support للمنتج A هي 0.6 و المنتج C هي 0.8. النتيجة النهائية للـLift هي 
   يساوي نتيجة Support لدينا وهي 0.4 تقسم على نتيجة الـSupport لكل منتج على حدة، للمنتج A النتيجة 0.6 وللمنتج C النتيجة 0.8. النتيجة النهائية للـLift هي:
   0.4 ÷ (0.6 * 0.8) = 0.833  

من الخوارزميات التي تنحدر أسفل Association Rules هي Apriori Algorithm، تقوم الخوارزمية بإكتشاف القيم الأكثر تكراراً. وتعمل كالتالي:  

![](https://alioh.github.io/images/2019-2-13/6.jpg)  

  * لنترض اننا وضعنا الحد Threshold يساوي 1.
  * نحسب تكرار كل قيمه.
  * يتم جمع القيم الأعلى من العملية الأخيره مع باقي القيم التي مجموع تكرارها أكبر من 1.
  * نكرر العملية الأخيره مره أخرى إلى ان تتبقى لنا قيمة واحده.  

مثال آخر [هنا](https://www.kdnuggets.com/2016/04/association-rules-apriori-algorithm-tutorial.html/2)  



[^1]: <https://uxdesign.cc/an-intro-to-machine-learning-for-designers-5c74ba100257>
[^2]: <https://towardsdatascience.com/clustering-unsupervised-learning-788b215b074b>
[^3]: <https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68>
[^4]: <https://www.saedsayad.com/association_rules.htm>
https://chatbotsmagazine.com/lets-know-supervised-and-unsupervised-in-an-easy-way-9168363e06ab
https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294

