---  
layout: post
title: ملخص كورس علم البيانات - 15
---  

آخر جزء في التعلم غير الموجَّه يتحدث عن Random Projection و ICA وهي طرق لتقليل المتغيرات.  
  


##### الفصل الثالث - التعلم غير الموجَّه Unsupervised Learning  
##### الدرس الخامس - Random Projection & ICA  
هي حل بديل لـPCA لتقليل المتغيرات. هدف الطريقة الرئيسي هو تغليل الـDimensions أو المتغيرات لدينا. لنفترض أن لدينا قاعدة بيانات فيها 12000 عامود أو متغير، عند إدخالها في Scikit-learn وتشغيل دالة Random Projection عليها ستعود لنا البيانات بعدد عواميد حوالي 6000. 

##### مثال بايثون  
طريقة عمل Random Projection في بايثون و Scikit-learn
```python
from sklearn import random_projection

data = data

#   إنشاء المودل الخاص ب Random Projection
#   من الممكن تحديد عدد n_components عدد المتغيرات المطلوب
#   وتحديد قيمة eps ε 
#   اذا تركت بدون تحديد فتكون قيمة eps = 0.1 و n_components = 'auto'
#   وهي القيم التلقائية
rp = random_projection.SparseRandomProjection()

#   تشغيل المودل على البيانات لدينا
new_data = rp.fit_transform(data)
```

##### Independent Component Analysis - ICA  
طريقة أخرى مشابهه لـ PCA و Random Projection مثال عليها في ملفات الصوت، اذا كان لدينا ثلاث ادوات صوتية تعمل بنفس الوقت، تشغيل هذه الخوارزمية عليها ستحاول فصل كل اداة لوحدها.  

##### مثال بايثون  
طريقة عمل Random Projection في بايثون و Scikit-learn
```python
from sklearn.decomposition import FastICA

#   البيانات لدينا متكونة من قائمة تحتوي على 3 ملفات صوتية
data = list(zip(audio_1, audio_2, audio_3))

#   إنشاء المودل الخاص
#   تحديد عدد n_components عدد المتغيرات المطلوب

ica = FastICA(n_components=3)

#   تشغيل المودل على البيانات لدينا
new_data = ica.fit_transform(data)
```

-----
[العودة إلى ملخص كورس علم البيانات - 14](https://alioh.github.io/DSND-Notes-14/)
  