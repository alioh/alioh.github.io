---  
layout: post
title: Ù…Ù„Ø®Øµ ÙƒÙˆØ±Ø³ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - 1
icon: ðŸ“
---  

Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù…Ù† Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø³ØªÙƒÙˆÙ† Ù…Ù„Ø®ØµØ§Øª Ø®Ø§ØµÙ‡ Ø¨ÙŠ ÙƒØªØ¨ØªÙ‡Ø§ Ø£Ø«Ù†Ø§Ø¡ Ø¯Ø±Ø§Ø³ØªÙŠ Ù„ÙƒÙˆØ±Ø³ [Udacity ÙÙŠ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](https://www.udacity.com/course/data-scientist-nanodegree--nd025) Ø£Ø­Ø¨Ø¨Øª Ø£ÙÙŠØ¯ ÙÙŠÙ‡Ø§ Ø§Ù„Ù…Ù‡ØªÙ…ÙŠÙ† ÙˆÙ…Ù† ÙŠØ±ØºØ¨ Ø¨Ø¯Ø®ÙˆÙ„ Ù…Ø¬Ø§Ù„ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙŠØªÙˆÙ‚Ø¹ Ù…Ù†Ùƒ Ø£Ù† ØªÙƒÙˆÙ† Ù„Ø¯ÙŠÙƒ Ù…Ø¹Ø±ÙØ© Ø³Ø§Ø¨Ù‚Ø© Ø¨Ù„ØºØ© Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¨Ø§ÙŠØ«ÙˆÙ† ÙˆØ¨Ø¹Ø¶ Ù…ÙƒØ§ØªØ¨Ù‡Ø§ Ø§Ù„Ù…Ø®ØµØµÙ‡ Ù„Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø«Ù„ scikit-learn.  

Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø£ÙˆÙ„ ÙŠØ´Ù…Ù„ Ø§Ù„ØªØ§Ù„ÙŠ:
- Machine Learning Bird's Eye View
- Linear Regression

##### Ø§Ù„ÙØµÙ„ Ø§Ù„Ø£ÙˆÙ„ - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ Supervised Learning  

##### Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø£ÙˆÙ„ - Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø© Ø¹Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Machine Learning Bird's Eye View  

ØªÙˆØ¬Ø¯ Ø«Ù„Ø§Ø« Ø§Ù†ÙˆØ§Ø¹ Ù…Ù† ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©


##### 1- Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ Supervised Learning  
ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ØŒ ØªØªØ¹Ù„Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙ†Ø§ ÙˆÙ…Ù† Ù†ØªØ§Ø¦Ø¬Ù‡Ø§ØŒ Ø¨Ø¹Ø¯ Ø£Ù† ØªØ¯Ø±Ø³Ù‡Ø§ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©ØŒ ØªØ³ØªØ·ÙŠØ¹ ØªÙˆÙ‚Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø£ÙŠ Ù‚ÙŠÙ…Ø© ØªÙ‚Ø¯Ù… Ù„Ù‡Ø§.  
![](https://alioh.github.io/images/2019-3-11/supervised.png)  
ÙŠÙ†Ù‚Ø³Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…ÙˆØ¬Ø© Ø¥Ù„Ù‰ Ù‚Ø³Ù…ÙŠÙ†:
- **Classification Ø§Ù„ØªØµÙ†ÙŠÙ**: ÙŠØ³ØªØ®Ø¯Ù… Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„ÙØ¦Ø© Ø§Ù„ØªÙŠ ØªÙ†ØªÙ…ÙŠ Ù„Ù‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ø£Ùˆ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ Ø®ÙŠØ§Ø±ÙŠÙ†. Ù…Ø«Ù„Ø§Ù‹ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ø²Ø¹Ø¬Ø© Ø£Ùˆ Ù„Ø§.  
- **Regression Ø§Ù„Ø¥Ù†Ø­Ø¯Ø§Ø±**: ØªØ³ØªØ®Ø¯Ù… Ù„ØªÙˆÙ‚Ø¹ Ù†ØªØ§Ø¦Ø¬ Ø±Ù‚Ù…ÙŠØ© Ù…Ø«Ù„ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ù†Ø§Ø²Ù„.  

Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ Ø³Ø¨Ù‚ Ø£Ù† Ø´Ø±Ø­ØªÙ‡ Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ [Ù‡Ù†Ø§](https://alioh.github.io/Machine-Learning-for-Everyone-3/).  

##### 2- Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ Unsupervised Learning  
ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØºÙŠØ± Ù…ÙˆØ¬Ù‘ÙŽÙ‡ØŒ ØªÙˆØ¬Ø¯ Ù„Ø¯ÙŠÙ†Ø§ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† Ù†ØªØ§Ø¦Ø¬ØŒ ÙØªØ­ØªØ§Ø¬ Ø§Ù„Ø¢Ù„Ø© Ù„Ù„ØªØ¹Ù„Ù… Ø¨Ù†ÙØ³Ù‡Ø§ØŒ ÙˆØªÙˆØ¬Ø¯ Ø£ÙƒØ«Ø± Ù…Ù† Ø·Ø±ÙŠÙ‚Ø© Ù…Ø«Ù„ ØªÙ‚Ø³ÙŠÙ… ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ ØªØ´Ø§Ø¨Ù‡Ù‡Ø§.  
Ø´Ø±Ø­ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„ Ø¹Ù† Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ [Ù‡Ù†Ø§](https://alioh.github.io/Machine-Learning-for-Everyone-4/)  

##### 3- Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ Reinforcement Learning  
ØªØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¹Ù…Ù„ Ù†Ø´Ø§Ø·Ø§Øª Ø£Ùˆ Actions ÙˆØªÙ„Ù‚ÙŠ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø´Ø§Ø·Ø§Øª Ø§Ù„ØªÙŠ ÙØ¹Ù„ØªÙ‡Ø§.  
ØªÙ… Ø´Ø±Ø­ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„ ÙÙŠ Ù…Ù†Ø´ÙˆØ± Ø³Ø§Ø¨Ù‚ [Ù‡Ù†Ø§](https://alioh.github.io/Machine-Learning-for-Everyone-5/)  

------------
##### Ø§Ù„ÙØµÙ„ Ø§Ù„Ø£ÙˆÙ„ - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙŽÙ‡ Supervised Learning  
##### Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø«Ø§Ù†ÙŠ - Ø§Ù„Ø¥Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Linear Regression  

![](https://alioh.github.io/images/2019-3-11/house.png) [^1]
ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù†Ø±ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ØŒ ÙƒÙ… Ø³ÙŠÙƒÙˆÙ† Ø³Ø¹Ø± Ø§Ù„Ø¨ÙŠØªØŒ Ø¨Ø¹Ø¯ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØª Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¨ÙŠÙˆØªØŒ ÙƒÙ… ØªØªÙˆÙ‚Ø¹ ÙŠÙƒÙˆÙ† Ø³Ø¹Ø±Ù‡ØŸ
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© 120 Ø£Ù„ÙØŒ ÙˆØ£Ø¬Ø¨Ù†Ø§ Ø¹Ù„ÙŠÙ‡ Ø¨ÙˆØ§Ø³Ø·Ø© Ø±Ø³Ù… Ø®Ø· Ù…Ù†Ø§Ø³Ø¨ Ù„ÙƒÙ„ Ø§Ù„Ù†Ù‚Ø§Ø· (Ø§Ù„Ø¨ÙŠÙˆØª) Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙ†Ø§ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

ØªÙˆØ¬Ø¯ Ø·Ø±ÙŠÙ‚ØªÙŠÙ† Ù„Ø±Ø³Ù… Ø®Ø· Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø·ØŒ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù‡ÙŠ Ø¹Ø¨Ø± Ø¥Ø³ØªØ®Ø¯Ø§Ù… Tricks Ø§Ùˆ Ø·Ø±Ù‚ Ù„ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ø®Ø·ØŒ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø³ØªØ®Ø¯Ø§Ù… Error Functions ÙˆÙ‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø®Ø·.

### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Tricks
Ø§Ù„Ù‡Ø¯Ù Ù…Ù† Ø±Ø³Ù… Ø§Ù„Ø®Ø· Ù‡Ùˆ Ø§Ù† ÙŠÙƒÙˆÙ† Ù‚Ø±ÙŠØ¨ Ø¬Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆÙ…Ù†Ø§Ø³Ø¨ Ù„Ù‡Ù… Ø¬Ù…ÙŠØ¹Ø§Ù‹ØŒ Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ ØªÙˆØ¬Ø¯ Ø·Ø±Ù‚ ÙƒØ«ÙŠØ±Ù‡ Ù„Ø±Ø³Ù… Ø§Ù„Ø®Ø· ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù†Øª Ù„Ø¯ÙŠÙ†Ø§ Ù†Ù‚Ø§Ø· ÙƒØ«ÙŠØ±Ù‡ ÙˆÙ‚Ø¯ ÙŠÙ†Ø§Ø³Ø¨ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆÙ„ÙƒÙ† Ù„Ù† ÙŠÙ†Ø§Ø³Ø¨Ù‡Ø§ Ø¬Ù…ÙŠØ¹Ù‡Ø§ØŒ Ø§Ù„Ù‡Ø¯Ù Ù‡Ù†Ø§ Ø±Ø³Ù… Ø®Ø· ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¬Ù…ÙŠØ¹.
ÙˆÙŠÙˆØ¬Ø¯ Ø·Ø±ÙŠÙ‚ØªÙŠÙ† Ù„ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ø®Ø· [^2]:
![](https://alioh.github.io/images/2019-3-11/movingline.png)  

### 1- Absolute Trick  
![](https://alioh.github.io/images/2019-3-11/movingline1.png)  
Ø§Ù„Ù‚ÙŠÙ…Ø© Î± Ù‡Ù†Ø§ ØªØ¹Ø±Ù Ø¨Ø§Ù„Ù€ Learning rate. Ù†Ù‚ÙˆÙ… Ø¨ØªØºÙŠØ±Ø© Ù„ÙŠØªØºÙŠØ± Ù…ÙƒØ§Ù† Ø§Ù„Ø®Ø·.  
( If the point is below the line, the intercept decreases (subtract from w1 and w2); if the point has a negative x-value, the slope increases. )

### 2- Square Trick  
![](https://alioh.github.io/images/2019-3-11/movingline2.png)  
Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ø³Ø§Ø¨:  
Ù†ÙØ±Ø¶ Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„Ù†Ù‚Ø·Ø©: (x,y) = (-5, 3)  
ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ù‡ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ù†ÙŠ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø®Ø· Ù„Ø¯ÙŠÙ†Ø§: y = -0.6x + 4  Ùˆ Î±=0.01
ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© q ØªØ¹Ù†ÙŠ Ù‡Ù†Ø§ y Ùˆ q' Ù†ØªÙŠØ¬Ø© Ø·Ø±Ø­ q-q'.  
Ù„Ø¥Ø³ØªØ®Ø§Ø±Ø¬ y Ù„Ù„Ø®Ø· Ù†Ù‚ÙˆÙ… Ø¨ØªØ¹ÙˆÙŠØ¶ x Ù…Ù† Ø§Ù„Ù†Ù‚Ø·Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© y = -0.6x + 4  
y = -0.6 (-5)+4 = 7  
Ø§Ù„Ø¢Ù† q-q' = 3 - 7 = -4  
Ù†Ø¹ÙˆØ¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù†Ø§ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©  
y = (W1 + p (q - q')Î±) x + (W2 + (q - q')Î±)  
y = (-0.6+(-5 * -4 * 0.01))x + (4 + (-4 *0.01))
y = (-0.6+0.2)x + 3.96
y = -0.4x + 3.96


---------


### Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Gradient Descent
Ø·Ø±ÙŠÙ‚Ø© Ù„Ø±Ø³Ù… Ø§Ù„Ø®Ø·ØŒ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© ØªØ­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ø§Ù„Ø®Ø· ÙˆØ§Ù„Ù†Ù‚Ø§Ø·ØŒ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø³Ø¨Ù‡ Ù‡ÙŠ Ù…Ø§ ÙŠØ³Ù…Ù‰ Ø¨Ø§Ù„Ù€Error ÙˆÙ‡Ùˆ Ù…Ø¬Ù…ÙˆØ¹ ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† ÙƒÙ„ Ù†Ù‚Ø·Ø© ÙˆØ§Ù„Ø®Ø·.  
ÙˆÙ‡Ø¯Ù Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù‡ÙŠ Ø­Ø³Ø§Ø¨Ù‡ ÙÙŠ ÙƒÙ„ Ù…Ø±Ù‡ ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªØ¬ÙŠØ© Ø£Ù‚Ù„ Ù…Ø§ ÙŠÙ…ÙƒÙ†.

Ø·Ø±ÙŠÙ‚ØªÙŠÙ† Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€Error:

### 1- Mean Absolute Error
Ù…Ø«Ø§Ù„:  
Ù„Ø¯ÙŠÙ†Ø§ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø· y = 1.2x + 2  
ÙˆØ§Ø¹Ø·ÙŠÙ†Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø§Ø·:  
(2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)  
ÙˆØ·Ù„Ø¨ Ù…Ù†Ø§ Ø­Ø³Ø§Ø¨ Mean Absolute ErrorØŒ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ù„Ø­Ø³Ø§Ø¨Ù‡ Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ù„Ø®Ø· ØªÙ‚Ø³ÙŠÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø·:  
Ø§ÙˆÙ„Ø§Ù‹ Ù†ÙˆØ¬Ø¯ Ø§Ù„Ù…Ø³Ø§ÙÙ‡ Ø¨ÙŠÙ† ÙƒÙ„ Ù†Ù‚Ø·Ø© ÙˆØ§Ù„Ø®Ø· ÙˆØ§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù‡ÙŠ Ø§ÙˆÙ„Ø§Ù‹ Ø¥ÙŠØ¬Ø§Ø¯ y' Ø¨ÙˆØ§Ø³Ø·Ø© Ø­Ù„ y ÙÙŠ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø®Ø·:  
Ù…Ø«Ø§Ù„ Ù„Ø£Ø­Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· (-4, -4):  
y = 1.2x + 2  
y = 1.2*(-4) + 2  
y = -4.8 + 2  
y = -2.8  
ÙˆÙ„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙÙ‡ Ø¨ÙŠÙ† Ø§Ù„Ù€y Ø§Ù„Ø®Ø· Ùˆ y Ø§Ù„Ù†Ù‚Ø·Ø© Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„ØªØ§Ù„ÙŠ:  
Error = -2.8 - -4 = 1.2  
Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„Ø³Ø§Ù„Ø¨ Ù†Ø£Ø®Ø° Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©.  
Ø§Ù„Ù†ØªÙŠØ¬Ø©:  
(6.4 + 2 + 1.2 + 7.4 + 2.4) / 5 = 3.88  

### 2- Mean Squared Error
Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø© ØªØ±Ø³Ù… Ù…Ø±Ø¨Ø¹Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø³Ø§ÙÙ‡ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù‚Ø·Ø© ÙˆØ§Ù„Ø®Ø·ØŒ ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© ØªÙƒÙˆÙ† Ø¨Ø­Ø³Ø§Ø¨ Ù…Ø³Ø§Ø­Ø© ÙƒÙ„ Ù…Ø±Ø¨Ø¹ ÙˆØ£Ø®Ø° Ø§Ù„Ù…ØªÙˆØ³Ø·.  
Ù†Ø£Ø®Ø° Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚:  
(6.4, 2, 1.2, 7.4, 2.4) ÙˆÙ†Ø±Ø¨Ø¹ ÙƒÙ„ ÙˆØ§Ø­Ø¯Ø©  
(40.96, 4, 1.44, 54.76, 5.76) Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù†Ø¶Ø±Ø¨ Ù‚Ø³Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø·:  
(40.96 + 4 + 1.44 + 54.76 + 5.76) / 5 = 106.92 / 5 = 21.38  


---------

## Ø£Ù…Ø«Ù„Ø© Ø¨Ø§ÙŠØ«ÙˆÙ† Ùˆ Ù…ÙƒØªØ¨Ø© scikit-learn  

### Linear Regression  
Ø§Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙ†Ø§ Ù…ØªØºÙŠØ± Feature ÙˆØ§Ø­Ø¯Ø©

```python
from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd

bmi_life_data = pd.DataFrame.from_csv('data.csv')

bmi_life_model = LinearRegression()
bmi_life_model.fit(x_values, y_values)
laos_life_exp = bmi_life_model.predict([[21.07931]])
```
x_values = Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø±ØºØ¨ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù‡Ø§  
y_values = Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙŠ Ù†ØªÙ†Ø¨Ø£ Ø¹Ù†Ù‡Ø§


### Multiple Linear Regression  
```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston
 
boston_data = load_boston()
x = boston_data['data']
y = boston_data['target']

model = LinearRegression()
model.fit(x, y)
sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,
                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,
                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]
prediction = model.predict(sample_house)
```


### Polynomial Regression  
Ù†Ø³ØªØ®Ø¯Ù… Polynomial Features Ù„Ø£Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Features. [^3]
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

train_data = pd.read_csv('data.csv')
X = train_data['Var_X'].values.reshape(-1, 1)
y = train_data['Var_Y'].values

poly_feat = PolynomialFeatures(degree = 4)
X_poly = poly_feat.fit_transform(X)

poly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)
```


### Regularization
Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ù„Ø¯ÙŠÙ†Ø§ Ø£Ùˆ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† ÙØ±Ø· Ø§Ù„ØªØ®ØµÙŠØµ Overfitting [^4]

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso

train_data = train_data = pd.read_csv('data.csv', header = None)
X = train_data.iloc[:, :-1]
y = train_data.iloc[:,-1]

lasso_reg = Lasso()
lasso_reg.fit(X,y)

reg_coef = lasso_reg.coef_
print(reg_coef)
```


### Feature Scaling  
Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ ØªØ´Ø§Ø¨Ù‡Ù‡Ø§ØŒ Ùˆ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠØ¶Ø§Ù‹ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ø°Ø©. [^5]
- **Standardizing**: ØªØ³ØªØ®Ø¯Ù… Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø§Ø°Ù‡.  
- **Normalizing**: ØªØ­ÙˆÙ„ ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ø±Ù‚Ø§Ù… Ù…Ù† 0 Ø¥Ù„Ù‰ 1.  

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso

train_data = train_data = pd.read_csv('data.csv', header = None)

X = train_data.iloc[:, :-1]
y = train_data.iloc[:,-1]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

lasso_reg = Lasso()

lasso_reg.fit(X_scaled, y)

reg_coef = lasso_reg.coef_
print(reg_coef)
```

## Ù†Ù‚Ø·ØªÙŠÙ† Ù…Ù‡Ù…Ù‡ Ø¹Ù† Linear Regression

- Ø§Ù„Ø¥Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ ÙŠØ¸Ù‡Ø± Ù‚ÙˆØªÙ‡ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø·ÙŠØ© Ø£Ùˆ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø®Ø·.
- Ø§Ù„Ø¥Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ ÙŠØªØ£Ø«Ø± Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØºØ±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø®Ø§Ø±Ø¬Ù‡ Ø¹Ù† Ø§Ù„Ù…Ø£Ù„ÙˆÙ OutliersØŒ Ù…Ø«Ø§Ù„: [^6]
    ![](https://alioh.github.io/images/2019-3-11/outliers_effect.png)  
  
  -----
  [Ø§Ù„Ø¥Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ù„Ø®Øµ ÙƒÙˆØ±Ø³ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - 2](https://alioh.github.io/DSND-Notes-2/)  
  



[^1]: [elevenching](https://medium.com/@elevenching/machine-learning-introduction-d2a91294667e)
[^2]: [TowardsDataScience](https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba)
[^3]: [StackExchange](https://stats.stackexchange.com/questions/58739/polynomial-regression-using-scikit-learn>)
[^4]: [TowardsDataScience](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)
[^5]: [ScikitLearn](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)
[^6]: [R-Statistics](http://r-statistics.co/Outlier-Treatment-With-R.html)