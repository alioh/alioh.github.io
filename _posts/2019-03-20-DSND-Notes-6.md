---  
layout: post
title: Ù…Ù„Ø®Øµ ÙƒÙˆØ±Ø³ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - 6
icon: ğŸ“
---  

Ø§Ù„Ù…Ø®Ù„Øµ ÙŠØºØ·ÙŠ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ù…Ø¯Ù‰ ÙƒÙØ§Ø¦ØªÙ‡ØŒ Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡Ø© ÙˆØ§Ù„Ø·Ø±Ù‚ Ù„ØªØ­Ø³ÙŠÙ†Ù‡.  
  
  


##### Ø§Ù„ÙØµÙ„ Ø§Ù„Ø£ÙˆÙ„ - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙÙ‡ Supervised Learning  
##### Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø«Ø§Ù…Ù† - Ù…Ù‚Ø§ÙŠÙŠØ³ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Model Evaluation Metrics  

- **Train_test_split**: Ø§Ø³ØªØ®Ø¯Ù…Øª Ø·Ø±ÙŠÙ‚Ø© ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø«Ø§Ù†ÙŠ](https://alioh.github.io/DSND-Notes-2) Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‚Ø³Ù…ÙŠÙ†ØŒ Ù‚Ø³Ù… Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙ‚Ø³Ù… Ù„Ù„Ø¥Ø®ØªØ¨Ø§Ø±ØŒ ÙˆØªÙ… Ø¥Ø³ØªØ¹Ù…Ø§Ù„ train_test_split Ù„Ø°Ù„ÙƒØŒ Ù‡Ø°Ù‡ Ø£Ø­Ø¯Ù‰ Ø·Ø±Ù‚ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯Ù„. ÙˆÙ„Ø¯ÙŠÙ‡Ø§ Ø¨Ø¹Ø¶ Ø§Ù„Ø®ØµØ§Ø¦Øµ (Ù…ØªØºÙŠØ±Ø§Øª):  
    - **test_size**: Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø®ØªØ¨Ø§Ø±Ù‡Ø§ØŒ Ø§Ø°Ø§ Ø§Ø¹Ø·ÙŠØª 0.5ØŒ ÙØ§Ù†Ù‡Ø§ ØªØ£Ø®Ø° 50% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.  
    - **train_size**: Ù†ÙØ³ ØªØ¹Ø±ÙŠÙ test_size ÙˆÙ„ÙƒÙ† Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙˆÙ„Ø§ ÙŠØ­ØªØ§Ø¬ Ù„ØªØ¹Ø±ÙŠÙØ© ÙˆØ¥Ø¹Ø·Ø§Ø¦Ø© Ù‚ÙŠÙ…Ø© Ø§Ø°Ø§ ØªÙ… ØªØ¹Ø±ÙŠÙ ÙˆØ¥Ø¹Ø·Ø§Ø¡ Ù‚ÙŠÙ…Ø© Ù„Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø®ØªØ¨Ø§Ø±Ù‡Ø§.  
    - **random_state**: Ø¹Ù†Ø¯ ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØºÙŠØ±ØŒ Ù„Ø§ ØªØªØºÙŠØ± Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙˆÙ„Ø© ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© Ù†Ø´ØºÙ„ ÙÙŠÙ‡Ø§ Ø§Ù„Ù…ÙˆØ¯Ù„. Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ÙŠØ­Ø¯Ø¯ØŒ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© Ù†Ø´ØºÙ„ ÙÙŠÙ‡Ø§ Ø§Ù„Ù…ÙˆØ¯Ù„ØŒ ØªØªØºÙŠØ± Ù„Ù†Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµÙ„Ø© Ù„Ø¯ÙŠÙ†Ø§ Ø¨ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ø±Ø¨ ÙˆØ§Ù„ØªÙŠ ØªÙØ®ØªÙØ¨Ø±.  

##### Ù…Ø«Ø§Ù„ Ø¨Ø§ÙŠØ«ÙˆÙ†
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.cross_validation import train_test_split
import pandas as pd
import numpy as np

data = np.asarray(pd.read_csv('data.csv', header=None))
X = data[:,0:2]
y = data[:,2]

#   Test size is 25%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(y_test)

acc = accuracy_score(y_train, y_pred)
```
##### Ù‚ÙŠØ§Ø³Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Classification Metrics  

- **Confusion Matrix**: Ø¬Ø¯ÙˆÙ„ ÙŠØ¨ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯Ù„ØŒ ÙˆÙŠÙƒÙˆÙ† ÙƒØ§Ù„ØªØ§Ù„ÙŠ:  
![](https://alioh.github.io/images/2019-3-20/Confusion-matrix-example.png)  
Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø®Ø¶Ø± ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙŠØ¹Ù†ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªÙˆÙ‚Ø¹Ù‡Ø§ ÙˆÙƒØ§Ù†Øª Ù†ØªØ§Ø¦Ø¬Ù‡Ø§ ØµØ­ÙŠØ­Ù‡ØŒ Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø­Ù…Ø± ÙŠØ¹Ù†ÙŠ Ù†ØªØ§Ø¦Ø¬ ÙƒØ§Ù† ØªÙˆÙ‚Ø¹Ù‡Ø§ Ø®Ø§Ø·Ø¦.  
  
- **Accuracy**: Ø³Ø¨Ù‚ Ø§Ù† ØªØ­Ø¯Ø«Øª Ø¹Ù†Ù‡Ø§ ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø«Ø§Ù„Ø«](https://alioh.github.io/DSND-Notes-3)ØŒ ÙˆÙ‡Ùˆ ÙŠØ­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Øª Ø§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ÙØµÙ†Ù (Classifier) ØµØ­ÙŠØ­Ù‡. ÙˆØ·Ø±ÙŠÙ‚Ø© Ø­Ø³Ø§Ø¨Ù‡ Ù‡ÙŠ ØªØ§Ø¨Ø¹Ù‡ Ù„Ù„ **Confusion Matrix**ØŒ ÙˆØ·Ø±ÙŠÙ‚Ø© Ø­Ø³Ø§Ø¨Ù‡ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:  
`[True Positives + True Negaitve/(Total Data)]`  

- **Precision**: Ø³Ø¨Ù‚ Ø§Ù† ØªØ­Ø¯Ø«Øª Ø¹Ù†Ù‡Ø§ ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø«Ø§Ù„Ø«](https://alioh.github.io/DSND-Notes-3)ØŒ ÙˆÙ‚Ù„Ù†Ø§ Ø§Ù†Ù‡ ÙŠØ­Ø¯Ø¯ Ù†Ø³Ø¨Ø© Ù…Ø¯Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù…ÙØµÙ†Ù ÙÙŠ ØªÙˆÙ‚Ø¹Ø§ØªÙ‡ Ø§Ù„ØµØ­ÙŠØ­Ù‡.  
`[True Positives/(True Positives + False Positives)]`  

- **Recall(sensitivity)**: Ø£ÙŠØ¶Ø§Ù‹ Ø³Ø¨Ù‚ Ø§Ù† ØªØ­Ø¯Ø«Øª Ø¹Ù†Ù‡Ø§ ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø«Ø§Ù„Ø«](https://alioh.github.io/DSND-Notes-3)ØŒ ÙˆØªÙ‚ÙˆÙ… Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙˆÙ‚Ø¹Ù†Ø§Ù‡Ø§ Ø£Ù†Ù‡Ø§ ØµØ­ÙŠØ­Ù‡ Ù…Ù† Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù†Ù‡Ø§ ØµØ­ÙŠØ­Ù‡.  
`[True Positives/(True Positives + False Negatives)]`  

- **F1 Score**: ÙˆØ£ÙŠØ¶Ø§Ù‹ Ø³Ø¨Ù‚ Ø§Ù† ØªØ­Ø¯Ø«Øª Ø¹Ù†Ù‡Ø§ ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø«Ø§Ù„Ø«](https://alioh.github.io/DSND-Notes-3)ØŒ ÙˆÙ‡ÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„ÙƒÙØ§Ø¦Ø© ÙˆØ¯Ù‚Ø© Ø§Ù„Ù…ÙˆØ¯Ù„.  
`F1 Scode = 2 * ( (Precision * Recall) / (Precision + Recall) )`  
ÙˆÙŠÙØ¶Ù„ Ø¥Ø³ØªØ®Ø¯Ø§Ù…Ø© Ø¨Ø¯Ù„ **Percision** Ùˆ **Recall** ÙƒÙˆÙ†Ù‡Ù…Ø§ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙ‚Ø¯Ù…Ø§ Ù†ØªØ§Ø¦Ø¬ Ø®Ø§Ø·Ø¦Ø© ÙˆÙ„Ø§ ØªØ¯Ù„ ÙØ¹Ù„Ø§Ù‹ Ø¹Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…ÙˆØ¯Ù„. [^1] [^2]  

- **F-beta**: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø¹Ù†Ù‡Ø§ [Ù‡Ù†Ø§](http://www.marcelonet.com/snippets/machine-learning/evaluation-metrix/f-beta-score)ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¨Ø³Ø§Ø·Ø© Ø¥Ø°Ø§ Ø§Ø±Ø¯Ù†Ø§ Ø§Ù† Ù†ÙƒØ±Ø² Ø£ÙƒØ«Ø± Ø¹Ù„Ù‰ **Precision**ØŒ Ù†Ø¹Ø·ÙŠÙ‡Ø§ Ø±Ù‚Ù…Ø§Ù‹ Ø¨ÙŠÙ† 0 Ùˆ 1ØŒ ÙˆØ¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§Ù‡Ø§ Ø§Ù† ØªØ±ÙƒØ² Ø¹Ù„Ù‰ **Recall** Ø£ÙƒØ«Ø±ØŒ ÙÙ†Ø¹Ø·ÙŠÙ‡Ø§ Ø±Ù‚Ù…Ø§Ù‹ Ø£Ø¹Ù„Ù‰ Ù…Ù† 1.  

- **ROC Curve**: ÙˆÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ… Ù…Ø¯Ù‰ Ù‚Ø¯Ø±Ø© Ø§Ù„Ù…ÙˆØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹. [^3]  


##### Ù‚ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø¥Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ Regression Metrics  

Ø³Ø¨Ù‚ Ø£Ù† Ø´Ø±Ø­Øª Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø§Ø· ÙÙŠ [Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø£ÙˆÙ„](https://alioh.github.io/DSND-Notes-1) ÙˆÙ‡ÙŠ:
- **Mean Absolute Error**
- **Mean Squared Error**

##### Ø§Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©  
- When can you use the model
    - decision trees? both **regression** and **classification**
    - random forest? both **regression** and **classification**
    - adaptive boosting? both **regression** and **classification**
    - logistic regression? **classification**
    - linear regression? **regression**
- Which metric we should use if we want to:
    - We have imbalanced classes, which metric do we definitely not want to use? **accuracy**, **naive-bayes**
    - We really want to make sure the positive cases are all caught even if that means we identify some negatives as positives? **recall**, **svm**
    - When we identify something as positive, we want to be sure it is truly positive? **precision**, **ada-boost**
    - We care equally about identifying positive and negative cases? **f1-score**, **random-forest**
- Which model for each metric:
    - precision? **classification**
    - recall? **classification**
    - accuracy? **classification**
    - r2_score? **regression**
    - mean_squared_error? **regression**
    - area_under_curve? **classification**
    - mean_absolute_area? **regression**


##### Ø§Ù„ÙØµÙ„ Ø§Ù„Ø£ÙˆÙ„ - Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆØ¬Ù‘ÙÙ‡ Supervised Learning  
##### Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„ØªØ§Ø³Ø¹ - Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø¶Ø¨Ø· Training and Tuning  
ÙŠÙˆØ¬Ø¯ Ù†ÙˆØ¹ÙŠÙ† Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: [^4] [^5]  
![](https://alioh.github.io/images/2019-3-20/over-underfit.png)  
- **Underfitting ÙØ±Ø· Ø§Ù„ØªØ¹Ù…ÙŠÙ…**: Ø¹Ù†Ø¯Ù†Ø§ Ù„Ø§ Ù†Ø¯Ø±Ø¨ Ø§Ù„Ù…ÙˆØ¯Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ ÙˆØªØ¸Ù‡Ø± Ù†ØªØ§Ø¦Ø¬ Ø³ÙŠØ¦Ø© Ø¹Ù†Ø¯ Ù‚ÙŠØ§Ø³ Ø§Ø¯Ø§Ø¡Ø©. (High Bias)  
- **Overfitting ÙØ±Ø· Ø§Ù„ØªØ®ØµÙŠØµ**: Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…ÙˆØ¯Ù„ Ù„Ø¯ÙŠÙ†Ø§ Ù…Ø¯Ø±Ø¨ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ ÙˆØ¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ØŒ ÙˆÙŠØ¸Ù‡Ø± Ø£Ø¯Ø§Ø© Ø§Ù„ÙØ¹Ù„ÙŠ Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¹Ø·Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠÙƒÙˆÙ† ØºÙŠØ± Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ Ù†ÙØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø°ÙŠ Ù‚Ø§Ù… Ø¨Ù‡ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ø±Ø¨ Ø¹Ù„ÙŠÙ‡Ø§. (High Variance)  

##### Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Cross Validation  
![](https://alioh.github.io/images/2019-3-20/cross_validation.png)  
ÙˆØ¸ÙŠÙØªÙ‡Ø§ ÙƒØ§Ù„ØªØ§Ù„ÙŠØŒ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø«Ù„Ø§Ø« Ø§Ù‚Ø³Ø§Ù…ØŒ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø£ÙˆÙ„ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯Ù„ØŒ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„ØªØ¹Ø¯ÙŠÙ„ ÙˆØ¶Ø¨Ø· Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…ÙˆØ¯Ù„ØŒ ÙˆØ§Ù„Ù‚Ø³Ù… Ø§Ù„Ø£Ø®ÙŠØ± Ù„Ù„Ø¥Ø®ØªØ¨Ø§Ø±. [^6] [^7]  
ÙˆØ·Ø±ÙŠÙ‚Ø© ÙØ¹Ù„ Ø°Ù„Ùƒ Ø¹Ø¨Ø± Ø®Ø§ØµÙŠØ© K-FoldØŒ ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ù…Ø¹ÙŠÙ† Ù†Ø­Ø¯Ø¯Ø© Ù„Ù‡Ø§ØŒ Ø¨Ø¹Ø¯ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ù†Ø®ØªØ¨Ø± Ø§Ù„Ù…ÙˆØ¯Ù„ Ø¹Ù„Ù‰ ÙƒÙ„ ÙˆØ§Ø­Ø¯Ù‡ØŒ ÙˆÙÙŠ ÙƒÙ„ Ù…Ø±Ù‡ Ù†Ù‚ÙˆÙ… Ø¨ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø¯Ø±Ø¨Ù‡Ø§ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø®ØªØ¨Ø±Ù‡Ø§.  [^8] [^9]  
![](https://alioh.github.io/images/2019-3-20/K-Fold-CV.png)  

### Ù…Ø«Ø§Ù„ ÙƒÙˆØ¯ Ø¨Ø§ÙŠØ«ÙˆÙ†  
```python
from sklearn.model_selection import KFold

X = np.array([[1, 2, 7], [3, 4, 6], [1, 2, 0], [3, 4, 9]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2, shuffle=True)
kf.get_n_splits(X)
#   returns 2

#   to check that it is different each iteration
for train_index, test_index in kf.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
```

##### Learning Curves  
Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¯Ù„ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† OverfittingØŒ UnderfittingØŒ Ø§Ùˆ Ù…Ù†Ø§Ø³Ø¨. Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠØŒ Ù…Ø«Ø§Ù„: [^10] [^11]  
![](https://alioh.github.io/images/2019-3-20/learning-curves.png)  
Ø§Ù„Ø®Ø· Ø§Ù„Ø£Ø®Ø¶Ø± Ù‡Ù†Ø§ Ù‡ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ù„Ø®Ø· Ø§Ù„Ø£ØµÙØ± Ù‡ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø®ØªØ¨Ø§Ø±. ÙˆÙƒÙ…Ø§ ÙŠØªØ¶Ø­ Ø£Ù† Ø§ÙØ¶Ù„Ù‡Ø§ Ù‡ÙŠ Ø§Ù„ÙˆØ³Ø·Ù‰ ÙƒÙˆÙ† Ø§Ù„Ø®Ø·ÙŠÙ† ÙŠÙ‚ØªØ±Ø¨Ø§Ù† Ø£ÙƒØ«Ø± ÙØ£ÙƒØ«Ø± Ù…Ù† Ø¨Ø¹Ø¶Ù‡Ù…Ø§ Ø§Ù„Ø¨Ø¹Ø¶. Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± Ù‡Ùˆ Ù…Ø«Ø§Ù„ Ù„Ù€**Underfitting** ÙˆØ¹Ù„Ù‰ Ø§Ù„ÙŠÙ…ÙŠÙ† Ù…Ø«Ø§Ù„ Ù„Ù€**Overfitting**
```python
train_sizes, train_scores, test_scores = learning_curve(
    estimator, X, y, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))
```
ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙÙŠÙ‡ Ù‡ÙŠ:
- estimator: ÙˆÙ‡ÙŠ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ø§Ù†Ù‡Ø§ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯Ù„. Ù…Ø«Ù„Ø§Ù‹ LogisticRegression.  
- X, y: ÙˆÙ‡ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.  
- train_sizes: Ø­Ø¬Ù… ÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ Ø³ØªØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ.  
- train_scores: Ø¯Ø±Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ù†Ø®ØªØ¨Ø±Ù‡Ø§ ÙÙŠÙ‡Ø§.  
- test_scores: Ø¯Ø±Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªÙŠ Ù†Ø®ØªØ¨Ø±Ù‡Ø§ ÙÙŠÙ‡Ø§.  

##### Grid Search  
Ø´Ø±Ø­Ù†Ø§ ÙÙŠ Ø£ÙƒØ«Ø± Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª HyperparametersØŒ ÙˆØ¸ÙŠÙØ© Grid Search Ù‡ÙŠ Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ ØªÙƒÙˆÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª.

##### Ù…Ø«Ø§Ù„ Ø¨Ø§ÙŠØ«ÙˆÙ† [^12] [^13] 
```python
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV


clf = DecisionTreeClassifier(random_state=42)
parameters = {'max_depth':[2,3,4,5], 'min_samples_leaf':[5,10,15], 'min_samples_split':[5,10,15]}
scorer = make_scorer(f1_score)
grid_obj = GridSearchCV(clf, parameters, scorer)
grid_fit = grid_obj.fit(X_train, y_train)
best_clf = grid_fit.best_estimator_
```
ÙˆØ§Ù„Ù†ØªÙŠØ¬Ø© Ø³ØªÙƒÙˆÙ† Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:  
```python
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=2, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=42,
            splitter='best')
```

##### Ø£ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø£Ø®ØªØ§Ø±ØŸ  
Ø¹Ù†Ø¯ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø£ÙŠ Ù…Ø´ÙƒÙ„Ø© ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ¬Ø±Ø¨Ø© Ø£ÙƒØ«Ø± Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ÙˆØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙØ¶Ù„ Ø¨ÙŠÙ†Ù‡Ù…ØŒ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ¹Ø·ÙŠ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù…ØªÙ‰ ÙˆØ£ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ®ØªØ§Ø±: [^14] [^15]  
![](https://alioh.github.io/images/2019-3-20/ml_map.png)  
![](https://alioh.github.io/images/2019-3-20/SLAT.png)  


-----
[Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ù…Ù„Ø®Øµ ÙƒÙˆØ±Ø³ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - 5](https://alioh.github.io/DSND-Notes-5/)   -   [Ø§Ù„Ø¥Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ù…Ù„Ø®Øµ ÙƒÙˆØ±Ø³ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - 7](https://alioh.github.io/DSND-Notes-7)  
  
  
[^1]: [TowardsDataScience](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
[^2]: [Exsilio](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)
[^3]: [TowardsDataScience](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)
[^4]: [MachineLearningMastery](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)
[^5]: [MachineLearningMedium](https://machinelearningmedium.com/2017/09/08/overfitting-and-regularization/)
[^6]: [codeproject](https://www.codeproject.com/articles/1146582/WebControls/%2FArticles%2F1146582%2FIntroduction-to-Machine-Learning)
[^7]: [NorwegianCreations](https://www.norwegiancreations.com/2018/10/artificial-intelligence-machine-learning-from-supervised-learning/)
[^8]: [TowardsDataScience](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)
[^9]: [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/)
[^10]: [ChrisAlbon](https://chrisalbon.com/machine_learning/model_evaluation/plot_the_learning_curve/)
[^11]: [Toptal](https://www.toptal.com/machine-learning/supervised-machine-learning-algorithms)
[^12]: [MachineLearningMastery](https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/)
[^13]: [ChrisAlbon](https://chrisalbon.com/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search/)
[^14]: [scikit-learn](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
[^15]: [VinodsBlog](https://vinodsblog.com/2018/04/02/supervised-machine-learning-insider-scoop-for-labeled-data/)